 \documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{inputenc}
\usepackage{graphicx}
\usepackage{titletoc} 
\usepackage{fancyhdr}   
\usepackage[a4paper,pdftex]{geometry}	
\usepackage[english]{babel}
\usepackage{xcolor} 
\usepackage{enumerate}
\usepackage{fix-cm} 
\usepackage[notlof]{tocbibind}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{float}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}
\definecolor{vgreen}{RGB}{104,180,104}
\definecolor{vblue}{RGB}{49,49,255}
\definecolor{vorange}{RGB}{255,143,102}
\renewcommand\lstlistingname{Appendix}
\renewcommand\lstlistlistingname{Appendix}

\makeatletter
\newcommand*\@lbracket{[}
\newcommand*\@rbracket{]}
\newcommand*\@colon{:}
\newcommand*\colorIndex{%
	\edef\@temp{\the\lst@token}%
	\ifx\@temp\@lbracket \color{black}%
	\else\ifx\@temp\@rbracket \color{black}%
	\else\ifx\@temp\@colon \color{black}%
	\else \color{vorange}%
	\fi\fi\fi
}
\makeatother

\usepackage{trace}

\usepackage{subcaption}
\begin{document}
	\begin{titlepage}
		\begin{center}
			\includegraphics[scale=.4]{Figures/Cover}\\
			\vspace{1cm}
			\bf{ \large {Department of Computer Science and Technology} }
		\end{center}
		
		\vspace{4cm}
		\centering
		\textbf{\Huge Machine Learning}
		\vspace{.5cm}
		
		{\Large Homework 3}

		\vspace{4cm}
		
		\textbf{\LARGE Sahand Sabour}
		
		
		
		\vspace{0.5cm}
		
		{\large 2020280401}
		
		
		\vfill
		
	\end{titlepage}


	\section{Clustering: Mixture of Multinomials}
	\subsection{MLE for multinomial}
	
	\noindent The likelihood function for this multinomial distribution is given as
	\begin{equation}
		P(x|\mu) = \frac{n!}{\prod_{i}x_i!}\prod_{i}\mu^{x_i}_i,\quad i=1, ..., d
	\end{equation}
	Taking log from both side of the above equation gives the log-likelihood function
	\begin{equation}
		\mathcal{L}(\mu) = log(P(x|\mu)) = log(n!)- log(\prod_{i}x_i!) +log(\prod_{i}\mu^{x_i}_i)
	\end{equation}
	This can be considered a Lagrange problem with the constraint $\sum_{i}\mu_i=1$. Hence, the Lagrangian equation can be formulated as
	\begin{equation}
		\mathcal{L}(\mu) = log(n!)- log(\prod_{i}x_i!) +log(\prod_{i}\mu^{x_i}_i) - \lambda(\sum_{i}\mu_i-1)
	\end{equation}
where $\lambda$ is Lagrangian multiplier, giving
\begin{equation}
	\mathcal{L}(\mu) = log(n!)- \sum_{i}log(x_i!) + \sum_{i}{x_i}log(\mu_i) - \lambda(\sum_{i}\mu_i-1)
\end{equation}
	Taking the derivative of the equation with respect to $\mu_i$ and setting it to 0 gives
	\begin{equation}
		\frac{\partial \mathcal{L}}{\partial \mu_i}= \frac{\sum_{i}x_i}{\sum_{i}\mu_i} - \lambda = 0
	\end{equation}
	Hence, we get that
	\begin{equation}
		 \lambda= \frac{\sum_{i}x_i}{\sum_{i}\mu_i} = \frac{n}{1} = n
	\end{equation}
	Accordingly, we could derive the maximum-likelihood estimator $\mu_i$ as
	\begin{equation}
		\mu_i= \frac{x_i}{\lambda} = \frac{x_i}{n},\quad i=1, ..., d
	\end{equation}


	\subsection{EM for mixture of multinomials}
	
	\section{PCA}
	\subsection{Minimum Error Formulation}
	
	\noindent Assuming that we have a set of complete orthonormal basis $\{\mu_i\}$, where $i\in[1,p]$, we have that $\mu_i^T\mu_j=\delta_{ij}$ and each data point can be represented as $x_n=\sum_{i}a_{ni}\mu_i$. Accordingly, due to orthonormal property, we can get that
	\begin{equation}
		a_{ni}=x_n^T\mu_i
	\end{equation}
	Inserting this in the data point representation gives
	\begin{equation}
		x_n=\sum_{i}(x_n^T\mu_i)\mu_i
	\end{equation}
	For this approach, the aim is to formulate PCA as minimizing
	the mean-squared-error of a low-dimensional approximation of the given basis. Hence, we assume a low-dimensional approximation of the point representation as follows
	\begin{equation}
		\widetilde{x}_n=\sum_{i}^{d}z_{ni}+\sum_{i=d+1}^{p}b_i\mu_i \quad \text{where $b_i$ are constants for all points}
	\end{equation}
	Therefore, the best approximation is to minimize the following error
	\begin{equation}
		\underset{ U,z,b}{\min}  J:=\frac{1}{N}\sum_{n=1}^{N}||x_n-\widetilde{x}_n||^2
	\end{equation}
	Consequently, we have that
	\begin{align*}
		J = \frac{1}{N}\sum_{n=1}^{N}||x_n-\widetilde{x}_n||^2\qquad \qquad \quad\\= \frac{1}{N}\sum_{n=1}^{N} (x_n-\widetilde{x}_n)^T(x_n-\widetilde{x}_n) \quad \\
		= \frac{1}{N}\sum_{n=1}^{N} x_n^Tx_n-2x_n^T \widetilde{x}_n + \widetilde{x}_n^T\widetilde{x}_n
	\end{align*}
	\noindent Inserting equation 10 in the above equation and replacing $\widetilde{x}_n$ gives
	\begin{align*}
		J = \frac{1}{N}\sum_{n=1}^{N} x_n^Tx_n-2x_n^T (\sum_{i}^{d}z_{ni}\mu_i+\sum_{i=d+1}^{p}b_i\mu_i) \\+ (\sum_{i}^{d}z_{ni}\mu_i^T+ 
		\sum_{i=d+1}^{p}b_i\mu_i^T)(\sum_{i}^{d}z_{ni}\mu_i+\sum_{i=d+1}^{p}b_i\mu_i)
	\end{align*}
	
	\noindent Accordingly, for minimizing this error, we calculate the derivative with respect to $z$ and $b$ and set it to 0.
	
	\vspace{-0.3cm}
	\begin{align*}
		\frac{\delta J}{\delta z_{nj}} = \frac{1}{n}[ -2x_n^T\mu_j+\mu_j^T(\sum_{i}^{d}z_{ni}\mu_i + \sum_{i=d+1}^{p}b_i\mu_i) + (\sum_{i}^{d}z_{ni}\mu_i^T + \sum_{i=d+1}^{p}b_i\mu_i^T)\mu_j] = 0 \\ 
		\frac{\delta J}{\delta z_{nj}} = \frac{1}{n}[ -2x_n^T\mu_j + 2 \mu_j^T(\sum_{i}^{d}z_{ni}\mu_i + \sum_{i=d+1}^{p}b_i\mu_i)] = 0 \qquad \qquad \\ 
		2 \mu_j^T(\sum_{i}^{d}z_{ni}\mu_i + \sum_{i=d+1}^{p}b_i\mu_i)= 2x_n^T\mu_j \qquad \qquad\qquad \quad\\
		\sum_{i}^{d}z_{ni}\mu_j^T\mu_i + \sum_{i=d+1}^{p}b_i\mu_j^T\mu_i=  x_n^T\mu_j \qquad \qquad\qquad \quad \\
		\sum_{i}^{d}z_{ni}\delta ij + \sum_{i=d+1}^{p}b_i\delta ij = z_{ni}+0 = x_n^T\mu_i \qquad \qquad\qquad \quad
	\end{align*}
	Giving $z_{ni}= x_n^T\mu_i$ for $i\in [1,d]$. Similarly, we the derivative with respect to b

	\vspace{-0.3cm}
	\begin{align*}
		\frac{\delta J}{\delta b_{j}} = \frac{1}{n}\sum[ -2x_n^T\mu_j+\mu_j^T(\sum_{i}^{d}z_{ni}\mu_i + \sum_{i=d+1}^{p}b_j\mu_i) + (\sum_{i}^{d}z_{ni}\mu_i^T + \sum_{i=d+1}^{p}b_j\mu_i^T)\mu_j] = 0 \\ 
		\frac{\delta J}{\delta b_{j}} = \frac{1}{n}\sum[ -2x_n^T\mu_j + 2 \mu_j^T(\sum_{i}^{d}z_{ni}\mu_i + \sum_{i=d+1}^{p}b_j\mu_i)] = 0 \qquad \qquad \\ 
		\sum(\sum_{i}^{d}z_{ni}\mu_j^T\mu_i + \sum_{i=d+1}^{p}b_j\mu_j^T\mu_i)=  \sum x_n^T\mu_j \qquad \qquad\qquad \quad \\
		\sum b_j= Nb_j =\sum x_n^T\mu_j \quad \text{giving}\quad b_j = \sum \frac{1}{n}x_n^T\mu_j = \bar{x}^Tu_j\qquad\qquad\\
	\end{align*}
	Which in turn gives $b_i=\bar{x}^Tu_i$ for $i\in [d+1,p]$. Accordingly, from equation 9, we can get the displacement lines in the orthogonal subspace as follows
	\begin{equation}
		x_n - \widetilde{x}_n = \sum_{i=d+1}^{p}\{(x_n-\bar{x})^T\mu_i\}\mu_i
	\end{equation}
	Which produces the following optimization problem for error J
	\begin{equation}
		\underset{\mu_j}{min} J \quad \text{where} \quad \mu_i^T\mu_i=1
	\end{equation}
	Assuming d=1 (1-dimensional subspace) and p=2 (2-dimensional space), the optimization problem becomes
	\begin{equation}
		\underset{\mu_2}{min} J = \mu_2^TS\mu_2 \quad \text{where} \quad \mu_2^T\mu_2=1
	\end{equation}
	Which gives $S\mu_2 = \lambda_2 \mu_2$, meaning that $\mu_2$ should be chosen as the eigenvector that corresponds to the smaller eigenvalue. Accordingly, the principal subspace is chosen by the eigenvector of the larger eigenvalue.
	
	\section{Deep Generative Models: Class-conditioned VAE}

	
\end{document}