 \documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{inputenc}
\usepackage{graphicx}
\usepackage{titletoc} 
\usepackage{fancyhdr}   
\usepackage[a4paper,pdftex]{geometry}	
\usepackage[english]{babel}
\usepackage{xcolor} 
\usepackage{enumerate}
\usepackage{fix-cm} 
\usepackage[notlof]{tocbibind}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{float}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}
\definecolor{vgreen}{RGB}{104,180,104}
\definecolor{vblue}{RGB}{49,49,255}
\definecolor{vorange}{RGB}{255,143,102}
\renewcommand\lstlistingname{Appendix}
\renewcommand\lstlistlistingname{Appendix}

\makeatletter
\newcommand*\@lbracket{[}
\newcommand*\@rbracket{]}
\newcommand*\@colon{:}
\newcommand*\colorIndex{%
	\edef\@temp{\the\lst@token}%
	\ifx\@temp\@lbracket \color{black}%
	\else\ifx\@temp\@rbracket \color{black}%
	\else\ifx\@temp\@colon \color{black}%
	\else \color{vorange}%
	\fi\fi\fi
}
\makeatother

\usepackage{trace}

\usepackage{subcaption}
\begin{document}
	\begin{titlepage}
		\begin{center}
			\includegraphics[scale=.4]{Figures/Cover}\\
			\vspace{1cm}
			\bf{ \large {Department of Computer Science and Technology} }
		\end{center}
		
		\vspace{4cm}
		\centering
		\textbf{\Huge Machine Learning}
		\vspace{.5cm}
		
		{\Large Homework 3}

		\vspace{4cm}
		
		\textbf{\LARGE Sahand Sabour}
		
		
		
		\vspace{0.5cm}
		
		{\large 2020280401}
		
		
		\vfill
		
	\end{titlepage}


	\section{Clustering: Mixture of Multinomials}
	\subsection{MLE for multinomial}
	
	\noindent The likelihood function for this multinomial distribution is given as
	\begin{equation}
		P(x|\mu) = \frac{n!}{\prod_{i}x_i!}\prod_{i}\mu^{x_i}_i,\quad i=1, ..., d
	\end{equation}
	Taking log from both side of the above equation gives the log-likelihood function
	\begin{equation}
		\mathcal{L}(\mu) = log(P(x|\mu)) = log(n!)- log(\prod_{i}x_i!) +log(\prod_{i}\mu^{x_i}_i)
	\end{equation}
	This can be considered a Lagrange problem with the constraint $\sum_{i}\mu_i=1$. Hence, the Lagrangian equation can be formulated as
	\begin{equation}
		\mathcal{L}(\mu) = log(n!)- log(\prod_{i}x_i!) +log(\prod_{i}\mu^{x_i}_i) - \lambda(\sum_{i}\mu_i-1)
	\end{equation}
where $\lambda$ is Lagrangian multiplier, giving
\begin{equation}
	\mathcal{L}(\mu) = log(n!)- \sum_{i}log(x_i!) + \sum_{i}{x_i}log(\mu_i) - \lambda(\sum_{i}\mu_i-1)
\end{equation}
	Taking the derivative of the equation with respect to $\mu_i$ and setting it to 0 gives
	\begin{equation}
		\frac{\partial \mathcal{L}}{\partial \mu_i}= \frac{\sum_{i}x_i}{\sum_{i}\mu_i} - \lambda = 0
	\end{equation}
	Hence, we get that
	\begin{equation}
		 \lambda= \frac{\sum_{i}x_i}{\sum_{i}\mu_i} = \frac{n}{1} = n
	\end{equation}
	Accordingly, we could derive the maximum-likelihood estimator $\mu_i$ as
	\begin{equation}
		\mu_i= \frac{x_i}{\lambda} = \frac{x_i}{n},\quad i=1, ..., d
	\end{equation}


	\subsection{EM for mixture of multinomials}
	
	\section{PCA}
	\subsection{Minimum Error Formulation}
	
	\noindent Assuming that we have a set of complete orthonormal basis $\{\mu_i\}$, where $i\in[1,p]$, we have that $\mu_i^T\mu_j=\delta_{ij}$ and each data point can be represented as $x_n=\sum_{i}a_{ni}\mu_i$. Accordingly, due to orthonormal property, we can get that
	\begin{equation}
		a_{ni}=x_n^T\mu_i
	\end{equation}
	Inserting this in the data point representation gives
	\begin{equation}
		x_n=\sum_{i}(x_n^T\mu_i)\mu_i
	\end{equation}
	For this approach, the aim is to formulate PCA as minimizing
	the mean-squared-error of a low-dimensional approximation of the given basis. Hence, we assume a low-dimensional approximation of the point representation as follows
	\begin{equation}
		\widetilde{x}_n=\sum_{i}^{d}z_{ni}+\sum_{i=d+1}^{p}b_i\mu_i \quad \text{where b is constant for all i}
	\end{equation}
	Therefore, the best approximation is to minimize the following error
	\begin{equation}
		\underset{ U,z,b}{\min}  J:=\frac{1}{N}\sum_{n=1}^{N}||x_n-\widetilde{x}_n||^2
	\end{equation}
	Consequently, we have that
	\begin{align*}
		J = \frac{1}{N}\sum_{n=1}^{N}||x_n-\widetilde{x}_n||^2\qquad \qquad \quad\\= \frac{1}{N}\sum_{n=1}^{N} (x_n-\widetilde{x}_n)^T(x_n-\widetilde{x}_n) \quad \\
		= \frac{1}{N}\sum_{n=1}^{N} x_n^Tx_n-2x_n^T \widetilde{x}_n + \widetilde{x}_n^T\widetilde{x}_n
	\end{align*}
	\noindent Inserting equation 10 in the above equation and replacing $\widetilde{x}_n$ gives
	\begin{equation*}
		J = \frac{1}{N}\sum_{n=1}^{N} x_n^Tx_n-2x_n^T (\sum_{i}^{d}z_{ni}+\sum_{i=d+1}^{p}b_i\mu_i) + (\sum_{i}^{d}z_{ni}+\sum_{i=d+1}^{p}b_i\mu_i^T)(\sum_{i}^{d}z_{ni}+\sum_{i=d+1}^{p}b_i\mu_i)
	\end{equation*}
	
	\noindent Accordingly, for minimizing this error, we calculate the derivative with respect to $z$ and $b$ and set it to 0.
	
	\begin{equation}
		\frac{\delta J}{\delta z_}
	\end{equation}
	
	
	\section{Deep Generative Models: Class-conditioned VAE}

	
\end{document}