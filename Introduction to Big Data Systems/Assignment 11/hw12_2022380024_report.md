# Assignment 11

> Sahand Sabour - 山姆 - 2022380024

## Introduction

<div style="text-align: justify">In this assignment, we are tasked to leverage the halide programming language to implement scheduling for the dilated convolution algorithm.  </div>

## Task 1
<div style="text-align: justify">Similar to the provided <b>conv.cpp</b> and <b>matmul.cpp</b> implementations, we decided to combine the loops of initialization and update stages of dilated convolution by defining an output function. Originally, dilated conv loads 0 into all values with 4 nested for loops and assigns updates within a 7-dimensional for loop, which shares the first 4 for loops with the initialization. In contrast, the output only considers one 7-dimensional for loop for producing the initializations and updates, and consumes the calculated value. By leveraging this design decision, we only initialize values and calculate the updates as necessary within the for loop, thus combining the two nested for loops. This is similar to the multi-stage processing example of consumer and producer functions in <a src= "https://halide-lang.org/tutorials/tutorial_lesson_08_scheduling_2.html">Halide tutorials: lesson 8</a>, where the producer is calculated as needed by the consumer. Therefore, we declare and assign the output function as follows: </div>

```c++
Func dilated_conv("dilated_conv");
Func output("output"); // declare the output function
RDom r(0, CI, 0, KW, 0, KH);

dilated_conv(c, x, y, n) = 0.0f;
dilated_conv(c, x, y, n) += filter(c, r.y, r.z, r.x) * input(r.x, x + r.y * (dilation + 1), y + r.z * (dilation + 1), n);
output(c, x, y, n) = dilated_conv(c, x, y, n); // assign dilated conv
```

The difference between the calculated for loops before and after adding output is provided below:

```bash
# Original for loops (given by dilated conv.print_loop_nest())
produce dilated_conv:
  for n:
    for y:
      for x:
        for c:
          dilated_conv(...) = ...
  for n:
    for y:
      for x:
        for c:
          for r12 in [0, 2]:
            for r12 in [0, 2]:
              for r12 in [0, 127]:
                dilated_conv(...) = ...
                
# The new for loops (given by output.print_loop_nest())
produce output:
  for n:
    for y:
      for x:
        for c:
          produce dilated_conv:
            dilated_conv(...) = ...
            for r12 in [0, 2]:
              for r12 in [0, 2]:
                for r12 in [0, 127]:
                  dilated_conv(...) = ...
          consume dilated_conv:
            output(...) = ...
```

<div style="text-align: justify">Accordingly, we implemented a rather trivial set of scheduling primitives to parallelize the program and optimize the locality. Initially, we leverage the tile traversal technique, which is discussed in <a src="https://halide-lang.org/tutorials/tutorial_lesson_05_scheduling_1.html">Halide tutorials: Lesson 5</a>. That is, we split the domain in which for loops of c and x are calculated into smaller tiles (inner loops) and then iterate over such tiles (outer loops). The tiling operation involves two splits for c and x, respectively. In addition, it involves a reorder operation that separates the outer loops (c_out and x_out) from the inner loops (c_in and x_in). For brevity, we directly use the tile function as follows:</div>

```c++
// Declare Variables
Var c_out, c_in, x_out, x_in, tile_idx;
// Implement tile traversal for the calculation
output.tile(c, x, c_out, x_out, c_in, x_in, 64, 4);
```

<div style="text-align: justify">The above split factors (64 and 4) were chosen based on trial and error. Initially, we wanted to split c into 128 scanlines as the input channel's dimension is set as 128. However, we noticed that splitting by a factor of 64 yielded better results. Regarding x, we used the split factor of four as this value had been generally used in the tutorials for performing the split operation and it also yielded the fastest results in our runs. To increase the computation speed further, we vectorized the inner loops of c and x as cross-vector computation would be much faster than calculating the results one by one within the two nested for loops.</div>

```c++
// Vectorize the values within each tile for faster computation
output.vectorize(c_in);
output.vectorize(x_in);
```

<div style="text-align: justify">Next, we implemented another technique from the tutorials that helps speed up the parallelism of nested for loops. This technique leverages fusing, as this operation is useful for parallelizing across multiple dimensions without introducing nested parallelism, which could have poor performance in Halide.</div>

```c++
// Fuse the outer loops for parallelism of tile traversal
output.fuse(c_out, x_out, tile_idx);
output.parallel(tile_idx);
```

<div style="text-align: justify">Lastly, in order to further enhance the parallelism of this algorithm, we also parallelize the two outermost loops (y and n) across different threads.</div>

```c++
// Parallelize the outermost loops across threads for faster computation
output.parallel(y);
output.parallel(n);
```

<div style="text-align: justify">The final structure of the algorithm's loops (obtained via the print_loop_nest function) is provided below:</div>

```bash
produce output:
  parallel n:
    parallel y:
      parallel c.v3.v7:
        vectorized x.v6 in [0, 3]:
          vectorized c.v4 in [0, 63]:
            produce dilated_conv:
              dilated_conv(...) = ...
              for r12 in [0, 2]:
                for r12 in [0, 2]:
                  for r12 in [0, 127]:
                    dilated_conv(...) = ...
            consume dilated_conv:
              output(...) = ...
```

### Results

<div style="text-align: justify">We compared the runtime between our proposed scheduling and oneDNN with different dilation values and provide the results in the table below. The results are average runtimes of 5 consecutive runs.</div>

| Dilation |           Ours            |          oneDNN          |
| :------: | :-----------------------: | :----------------------: |
|    0     | 39.54ms (298.37 GFLOP/s)  | 31.53ms (374.24 GFLOP/s) |
|    15    | 41.48ms (284.41 GFLOP/s)  | 34.83ms (338.68 GFLOP/s) |
|    31    | 46.68ms (252.70 GFLOP/s)  | 38.88ms (303.42 GFLOP/s) |
|    63    | 54.78ms (215.337 GFLOP/s) | 45.67ms (258.31 GFLOP/s) |

<div style="text-align: justify">As shown by the above results, our trivial implementation performs comparatively well, with milisecond difference between the runtime of our approach and oneDNN. In addition, for both approaches, we can observe that with increasing dialtion, the computation time also increases. Interestingly, the amount of such increase in the runtime is nearly the same for both approaches.</div>

## Guide

<div style="text-align: justify">For running the program, simply compile and run the program as follows:</div>

```shell
make dilated_conv
srun -n 1 ./dilated_conv
```

Note that the dilation value must be changed within the provided source code. 