# Assignment 10

> Sahand Sabour - 山姆 - 2022380024

## Introduction

<div style="text-align: justify">In this assignment, we are tasksed to listen to a HDFS stream that prints 200 lines per minute, each line containing 5 words. Our task is to perform wordcount on the incoming stream and print the top-k frequent words (k=100). In addition, we should update wordcounts every minute based on the new incoming words.</div>

## Implementation

<div style="text-align: justify">The code that we are provided first creates a spark context and starts listening to the incoming stream every 60 seconds (i.e., one minute). Accordingly, since we need to consider results from previous minutes when calculating the wordcount for the current minute's input, we create checkpoints in the HDFS output directory. In addition, we maintain the provided mapreduce paradigm that creates (key, 1) tuples for each words (Map) and sums up all tuples with the same key (Reduce). We also add an updateStateByKey function that sums up the (key, value) pairs for the same keys of the current minute with the previous minute. Accordingly, we sort the results in the descending order, as required by the assignment guidelines.</div>

```python
# This function sums old and new wordcounts
def update_count(new_count, old_count):
    return sum(new_count, old_count if old_count else 0)
  
# Create spark context for the program
sc = SparkContext(appName="Py_HDFSWordCount")
# Listen to changes every 60 seconds
ssc = StreamingContext(sc, 60)
# Save checkpoints to keep track of every minute's wordcounts
ssc.checkpoint("hdfs://intro00:9000/user/2022380024/hw10output")
# Read the lines from the stream (current minute)
lines = ssc.textFileStream("hdfs://intro00:9000/user/2022380024/stream")
# Count the number of words (current minute + previous minute if exists)
counts = (
    lines.flatMap(lambda line: line.split(" "))  # Split lines into words
    .map(lambda x: (x, 1))  # Make tuples for each word
    .reduceByKey(lambda a, b: a + b)  # Sum up 1s of tuples with same key
    .updateStateByKey(update_count)  # Add up results from current and previous minutes
    .transform(lambda r: r.sortBy(lambda x: -x[1]))  # Sort RDDs in the descending order
)
```

<div style="text-align: justify">As mentioned, we are also required to print and save the top-100 frequent results for the minute. For printing, we leverage the pprint function as provided by the API. To save the results for each minute, we created a function that iterates through RDDs within a stream to create a global list of wordcounts and save the top-100 results. Since we could not implement a variable that tracks the minutes in the stream, we iterate from 1-5 (as program is required to run for 5 minutes and therefore, produce 5 output files) and see if corresponding output files exist to determine the current minute.</div>

```python
# This functions receives collected RDDs and saves them to a file
def save_res(res):
    if res:
        for i in range(5):
            if not os.path.exists(f"results_{i+1}.txt"):
                with open(f"results_{i+1}.txt", "w") as f:
                    for j in range(min(len(res), 100)):
                        f.write(f"{res[j][0]}: {res[j][1]}\n")
                f.close()
                break
                
# Print and save the top-100 most frequent words.
counts.pprint(100)
counts.foreachRDD(lambda rdd: save_res(rdd.collect()))

ssc.start()
ssc.awaitTermination()
```

<div style="text-align: justify">Since we are using checkpoints and dedicate the output directory to such checkpoints, we insert the following additions in the generator's script:</div>

```python
/hadoop/bin/hdfs dfs -mkdir /user/$username/hw10output/
/hadoop/bin/hdfs dfs -rm /user/$username/hw10output/*
```

## Guide

For running the program, first move to the `generator` and start the generator as follows:

```shell
sh ./generator.sh
```

Accordingly, move to the `simple_app_python` and start the program as follows:

```shell
sh ./submit.sh
```