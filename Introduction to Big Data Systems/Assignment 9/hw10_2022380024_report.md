# Assignment 9

> Sahand Sabour - 山姆 - 2022380024

## Introduction

<div style="text-align: justify">In this assignment, we were tasked to design a graph mining algorithm for frequent subgraph mining (FSM). Our designed algorithm was required to detect frequent patterns (pattern_size = 3) based on a simplified financial dataset. </div>


## Methodology 

### Data Processing

<div style="text-align: justify">A major part of our implemented algorithm is analyzing and processing the given dataset so that it could be used as the input for an FSM algorithm. In total, we are given four files: two files for vertices (card and account) and two files for edges (account-to-card and account-to-account). First, we combine all four files by gathering important information from the edges and looking up node names from the vertice files.</div>

```python
# Card
card_dict = {}
with open('data/card') as f:
    for line in f:
        id, name, _, _ = line.split(",")
        card_dict[id] = name

# Account
acc_dict = {}
with open('data/account') as f:
    for line in f:
        id, name, _, _ = line.split(",")
        acc_dict[id] = name

# Account-to-account        
df_acc = pd.read_csv('data/account_to_account', sep=",", names=["src", "tgt","_1", "amt", "strategy","_2", "bus","_3","_4","_5","_6","_7","_8"], header=None)
df_acc = df_acc[["src", "tgt", "amt", "strategy", "bus"]]

# Account-to-card
df_card = pd.read_csv('data/account_to_card', sep=",", names=["src", "tgt","_1", "amt", "strategy","_2", "bus","_3","_4","_5","_6","_7","_8"], header=None)
df_card = df_card[["src", "tgt", "amt", "strategy", "bus"]]

# Look up names in the corresponding files
df_card["src_name"] = df_card["src"].apply(lambda x: acc_dict[str(x)])
df_card["tgt_name"] = df_card["tgt"].apply(lambda x: card_dict[str(x)])
df_acc["src_name"] = df_acc["src"].apply(lambda x: acc_dict[str(x)])
df_acc["tgt_name"] = df_acc["tgt"].apply(lambda x: acc_dict[str(x)])

# Account to card is 0 and account to account is 1
df_card["type"] = 0
df_acc["type"] = 1

# Create one collective dataset
df = pd.concat([df_card, df_acc])
```

<div style="text-align: justify">Accordingly, to reduce the information load of the input, we first remove redundant sections such as "strategy_name-" in the strategy field and "buscode" in the bus field since their id is sufficient to distinguish between different strategies and buscodes. We also round the amt value to the nearest integer as required in this assignment. Then, we remove all duplicate edges (i.e., rows that are completely similar) as they result in duplicated of observed instances of a pattern and should not be considered in the pattern's frequency. </div>

```python
df["amt"] = df["amt"].apply(lambda x: int(float(x)))
df["strategy"] = df["strategy"].apply(lambda x: int(x.replace("strategy_name-", "")))
df["bus"] = df["bus"].apply(lambda x: int(x.replace("buscode", "")))

# Drop rows that are completely similar
df = df.drop_duplicates()
```

<div style="text-align: justify">Lastly, we create edge embeddings by concatenating information that should be considered in isomorphism and count their frequencies. The logic for this process is that is an edge embedding is present less than our support value (10000), it means that it cannot be part of an acceptable pattern. For example, if we only have 9000 instances of an edge embedding, then this edge embedding can never be part of a desired triangle (i.e., one of the three edge embeddings that makes a triangle pattern) since it simply is not frequent enough. Accordingly, we remove all edges whose embeddings are not sufficiently frequent (i.e., frequency < support).</div>

```python
# Make edge embeddings
df["edge"] = df["src_name"].astype(str) + "-" + df["tgt_name"].astype(str) + "-" + df["type"].astype(str) + "-" + df["strategy"].astype(str) + "-" + df["bus"].astype(str) + "-" + df["amt"].astype(str)

# Count the freuquency of edge embeddings
df["edge_count"] = df.groupby(['edge'])["src"].transform('count')

# Remove edges that are not frequent enough
df = df[df["edge_count"] >= 10000]
```

<div style="text-align: justify">This assumptions serves as the foundation of our approach and it efficiently reduces the runtime as it discards more than half of the edges in the dataset.</div>

### Frequent Subgraph Mining

<div style="text-align: justify">We implement a trivial yet effective solution to find the most frequent subgraphs in the given dataset. First, we read the graph data and and assign edges to source and destination vertices via a dictionary. For each vertice to serve as a key, we only consider its ID. Therefore, to not have the same key for cards and accounts, since we have 800,000 accounts and 600,000 cards in the dataset, we set the ID range for cards from [0-599999] to [800000-1399999]. Note that this transformation is only done on target vertices as all source vertices are accounts (i.e., either account-to-account or account-to-card). Accordingly, we store all important information in the edge and to ensure memory runtime efficiency, we leverage tuples rather than structs or objects.</div>

```python
def read_graph(file_name):
    with open(file_name) as input_file:
        rows = csv.reader(input_file, delimiter=",")
        graph = {}
        num_edges = 0
        # Skip the header
        next(rows, None)
        for row in rows:
            # Read row values
            src, dst, amt, strategy, bus, src_name, dst_name, node_type = row
            # Calculate destination id based on node type (card or account)
            src_v = int(src)
            dst_v = (1 - int(node_type)) * 800000 + int(dst)
            # Create edge tuple
            e = (
                src_v,
                dst_v,
                src_name,
                dst_name,
                f"{amt}-{strategy}-{bus}-{int(node_type)}",
            )

            # Save in and out edges
            graph[src_v] = graph.get(src_v, []) + [e]
            graph[dst_v] = graph.get(dst_v, []) + [e]

            num_edges += 1

    print(f"Graph loaded - # Vertices: {len(graph)} - # Edges: {num_edges}")

    return graph
```

<div style="text-align: justify">After loading the graph, we leverage 4 for loops to iterate over the graph, since the pattern size is 3 (i.e., patterns with 4 vertices and 3 edges). The first loop (the outermost loop) checks all the vertices in the graph and for each vertice <b>i</b>, finds its connected edges. Accordingly, in the second for loop (the first inner loop), we iterate through all the vertices <b>j</b> that are connected to <b>i</b>. Since we save the edges for both source and destination vertices when loading the graph, vertice <b>j</b> would be the edge's source if <b>i</b> is the destination, and vice versa. Then, in the third for loop (the second inner loop), we iterate over all the edges connected to <b>j</b>. Lastly, in the fourth and last for loop (the innermost loop), we iterate over all the edges connected to <b>k</b>. In all loops, we ensure that we are not using a duplicate edge (i.e., an edge that we have covered in the current iteration). In addition, in the fourth loop, we make a string sequence of sorted (src, dst) pairs for the 3 selected edges and check if this sequence has been used before. This is to ensure that we do not count duplicate instances of the same subgraph. For instance, if we have edges 1->2, 2->3, 3->1, then our sorted sequence would be "1-2_2-3_3-1". Therefore, a permutation of these edges, such as 3->1, 1->2, 2->3, should not be counted in the pattern's frequency as it is the same subgraph.</div>

```python
def make_edge_embedding(e):
    return "-".join(str(item) for item in e[2:])

def make_pattern(edges):
    return "_".join(sorted([make_edge_embedding(e) for e in edges]))
  
def find_frequent_subgraphs(file_name):
    graph = read_graph(file_name)
    patterns = {}
    used_subs = {}
    for i in tqdm(graph.keys()):
        adj_e_i = graph[i]
        for e_i in adj_e_i:
            j = e_i[1] if e_i[0] == i else e_i[0]
            adj_e_j = graph.get(j, [])
            for e_j in adj_e_j:
                if e_j != e_i:
                    k = e_j[1] if e_j[0] == j else e_j[0]
                    adj_e_k = graph.get(k, [])
                    for e_k in adj_e_k:
                        if e_k not in [e_i, e_j]:
                            es = [e_i, e_j, e_k]
                            cur_sub = "-".join(sorted([f"{e[0]}_{e[1]}" for e in es]))
                            # Checking if the subgraph has already been used.
                            if not used_subs.get(cur_sub, 0):
                                used_subs[cur_sub] = 1
                                p_key = make_pattern(es)
                                if not patterns.get(p_key, 0):
                                    p_val = {"frequency": 1, "orig": es}
                                    patterns[p_key] = p_val
                                else:
                                    patterns[p_key]["frequency"] += 1
```

<div style="text-align: justify">Furthermore, we create patterns in a similar process. However, since we follow the isomorphism rules to detect instances of patterns, we exclude the original vertice IDs and include names instead. However, we store the original form of the edges as they would be required for producing the output. Lastly, we save the discovered patterns in a json files, as required by the assignment guideline. Note that for cards, we have to switch the ID range back to [0-599999].</div>

```python
def make_json(patterns):
    res = []
    for v in patterns.values():
        if v["frequency"] >= 10000:
            obj = {"frequency": v["frequency"], "nodes": [], "edges": []}
            for i, e in enumerate(v["orig"]):
                obj["nodes"].append({"node_id": i, "name": e[2]})
                amt, strategy, bus, node_type = e[4].split("-")
                obj["edges"].append(
                    {
                        "source_node_id": e[0],
                        "target_node_id": e[1] - 800000 * (1 - int(node_type)),
                        "amt": int(float(amt)),
                        "strategy_name": strategy,
                        "buscode": bus,
                        "type": "account-to-account"
                        if int(node_type)
                        else "account-to-card",
                    }
                )
                if i == 2:
                    obj["nodes"].append({"node_id": i + 1, "name": e[3]})

            res.append(obj)

    with open("bdci_data.json", "w") as outfile:
        outfile.write(json.dumps(res, indent=2))
```

## Results

<div style="text-align: justify">After running our implemented algorithm (over approximately 5 hours), we found <b>15034</b> frequent subgraphs. We summarize the 10 discovered topologies of the subgraphs in the figure below.</div>

![graph](/Users/sahandsabour/Desktop/Tsinghua/Introduction to Big Data Systems/Assignments/Assignment 9/graph.png)

## Guide

<div style="text-align: justify">For running the program, simply compile and run the program as follows:</div>

1. Create a folder `/data` and put the 4 input files in this folder.
2. Run all the lines in the `main.ipynb` notebook to generate `data/full.csv`.
3. Run the following command to start the program:

```shell
python main.py
```

The results will be stored in `bdci_data.json`. 

For drawing the graph for the results, create a folder `/figures` and run the following command:

```shell
python graph_res.py
```